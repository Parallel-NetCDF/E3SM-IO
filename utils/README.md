## Utility Programs for E3SM-IO benchmark

* [dat2nc](#dat2nc) -- converts multiple decomposition files into a NetCDF file
* [datstat](#datstat) -- displays statistics of decomposition files.
* [pnetcdf_blob_replay](#pnetcdf_blob_replay) -- converts the subfiles produced
  by the `e3sm_io` benchmark when it ran with PnetCDF blob I/O strategy into a
  regular NetCDF file.

---
## dat2nc
**dat2nc** is a utility program that converts a PIO decomposition file (in a
text format) to a NetCDF file, which can be fed to `e3sm_io` benchmark to
evaluate the I/O performance.

### Convert the data decomposition file into NetCDF file
The instructions below explain how to convert the data decomposition file into
NetCDF file format.
* For the F case, there are three data decomposition files generated by the
  E3SM production runs, The files are in a text format with file extension name
  ".dat". Converting the decomposition files into a single NetCDF file enables
  the parallel reading. Similarly, for the G case, there are six decomposition
  files that need to be converted first.
* A utility program, `dat2nc.c`, can be used to convert the text files. Running
  command `make` under to root folder should already build the executable.
* In addition to converting the text-based decomposition map into binary form. 
  The dat2nc utility also converts the decomposition map into the format used by
  the E3SM benchmark.
  + 0 offsets are removed.
    In the PIO decomposition map, the index starts from 1. The decomposition map 
    may contain 0s which means the process did not access any element. They are 
    removed in the converted NetCDF file.
  + Access to consecutive locations is merged.
    The PIO decomposition map records the variable offset of every element accessed
    by a process. The dat2nc utility converts them into offset and length pairs, 
    merging consecutive elements.
  + The offset and length pairs are sorted according to the offset.
* The original decomposition map can be included in the NetCDF file by adding 
  option `-r`. When enabled, dat2nc will save a copy of the original decomposition 
  map in the binary form before applying the conversion mentioned above. It is used 
  in the ADIOS blob test cases.
* The command to combine the three `.dat` files to a NetCDF file for the F case
  as an example, is:
  ```
    % ./dat2nc -o outputfile.nc -1 decomp_1.dat -2 decomp_2.dat -3 decomp_3.dat
  ```
* Command-line options of `./dat2nc`:
  ```
    % ./dat2nc -h
    Usage: ./dat2nc [OPTION]... [FILE]...
          -h               Print help
          -v               Verbose mode
          -r               Include original decomposition map
          -l num           Max number of characters per line in input file
          -o out_file      Name of output NetCDF file
          -1 input_file    name of 1st decomposition file
          -2 input_file    name of 2nd decomposition file
          -3 input_file    name of 3rd decomposition file
          -4 input_file    name of 4th decomposition file
          -5 input_file    name of 5th decomposition file
          -6 input_file    name of 6th decomposition file
  ```
* Three small input decomposition files for an F case are provide in
  directory [../datasets/](../datasets/).
  * `piodecomp16tasks16io01dims_ioid_514.dat`  (decomposition along the fastest dimensions)
  * `piodecomp16tasks16io01dims_ioid_516.dat`  (decomposition along the fastest dimensions)
  * `piodecomp16tasks16io02dims_ioid_548.dat`  (decomposition along the fastest two dimensions)
* The NetCDF file converted from these 3 decomposition `.dat` files is also
  provided in the same folder `../datasets` with file named
  `f_case_866x72_16p.nc`. Its metadata is shown below.
  ```
    % cd ./datasets
    % ncmpidump -h f_case_866x72_16p.nc
    netcdf f_case_866x72_16p {
    // file format: CDF-1
    dimensions:
        num_decomp = 3 ;
        decomp_nprocs = 16 ;
        D1.total_nreqs = 47 ;
        D2.total_nreqs = 407 ;
        D3.total_nreqs = 29304 ;
    variables:
        int D1.nreqs(decomp_nprocs) ;
            D1.nreqs:description = "Number of noncontiguous requests per process" ;
        int D1.offsets(D1.total_nreqs) ;
            D1.offsets:description = "Flattened starting indices of noncontiguous requests" ;
        int D1.lengths(D1.total_nreqs) ;
            D1.lengths:description = "Lengths of noncontiguous requests" ;
            D1.lengths:max = 36 ;
            D1.lengths:min = 9 ;
        int D2.nreqs(decomp_nprocs) ;
            D2.nreqs:description = "Number of noncontiguous requests per process" ;
        int D2.offsets(D2.total_nreqs) ;
            D2.offsets:description = "Flattened starting indices of noncontiguous requests" ;
        int D2.lengths(D2.total_nreqs) ;
            D2.lengths:description = "Lengths of noncontiguous requests" ;
            D2.lengths:max = 4 ;
            D2.lengths:min = 1 ;
        int D3.nreqs(decomp_nprocs) ;
            D3.nreqs:description = "Number of noncontiguous requests per process" ;
        int D3.offsets(D3.total_nreqs) ;
            D3.offsets:description = "Flattened starting indices of noncontiguous requests" ;
        int D3.lengths(D3.total_nreqs) ;
            D3.lengths:description = "Lengths of noncontiguous requests" ;
            D3.lengths:max = 4 ;
            D3.lengths:min = 1 ;

    // global attributes:
        :command_line = "./dat2nc -o f_case_866x72_16p.nc -1 datasets/piodecomp16tasks16io01dims_ioid_514.dat -2 datasets/piodecomp16tasks16io01dims_ioid_516.dat -3 datasets/piodecomp16tasks16io02dims_ioid_548.dat " ;
        :D1.ndims = 1 ;
        :D1.dims = 866 ;
        :D1.max_nreqs = 4 ;
        :D1.min_nreqs = 2 ;
        :D2.ndims = 1 ;
        :D2.dims = 866 ;
        :D2.max_nreqs = 39 ;
        :D2.min_nreqs = 13 ;
        :D3.ndims = 2 ;
        :D3.dims = 72, 866 ;
        :D3.max_nreqs = 2808 ;
        :D3.min_nreqs = 936 ;
    }
  ```
* The NetCDF file containing the 6 decompositions from a small G case is also
  available in folder `../datasets` with file named `g_case_cmpaso_16p.nc`.

---
## datstat
**datstat** reads a decomposition file in its original text format and report
statistics of the decomposition.

* Command-line options of `datstat`:
  ```
    % ./datstat -h
    Usage: ./datstat [OPTION]...
           -h               Print help
           -d input_file    decomposition file to analyze
  ```

* Example run:
  ```
    % ./datstat -d ../datasets/piodecomp16tasks16io01dims_ioid_514.dat
    =========================================================================
    Deomposition file ../datasets/piodecomp16tasks16io01dims_ioid_514.dat
    =========================================================================
    version: 2001
    npes: 16
    ndims: 1
    Dims: 866,
    total_cells: 1536
    total_zero_cells: 670
    max_cells: 96 @ rank 0
    max_zero_cells: 56 @ rank 12
    zero ratio: 0.436198
  ```

---
## pnetcdf_blob_replay
**pnetcdf_blob_replay** reads the subfiles produced by the `e3sm_io` benchmark
into a single, regular NetCDF file. Currently, it must be run with the same
number of MPI processes that produced the subfiles. The future work will remove
such limitation to allow less number of MPI processes to convert.

* Command-line options of `pnetcdf_blob_replay`:
  ```
    % ./pnetcdf_blob_replay -h
      Usage: pnetcdf_blob_replay [OPTION]... FILE
          [-h] Print help
          [-v] Verbose mode
          [-i file] Base name of input subfiles
          [-o file] Output file name
  ```
* Example run:
  ```
    % mpiexec -n 16 ./pnetcdf_blob_replay -i blob_F_out_h0.nc -o F_out_h0.nc
      Input subfile base name            = blob_F_out_h0.nc
      Output file name                   = F_out_h0.nc
      No. decompositions                 =   3
      No. variables                      = 429
      No. records (time steps)           =   1
      Max Time of file open/create       = 0.0494 sec
      Max Time of define variables       = 0.0446 sec
      Max Time of read                   = 0.0090 sec
      Max Time of write                  = 0.3435 sec
      Max Time of close                  = 0.0249 sec
      Max end-to-end time                = 0.4714 sec
      -----------------------------------------------------------
      Total read  amount                 = 16.80 MiB = 0.02 GiB
      Read  bandwidth                    = 1869.3614 MiB/sec
      Total write amount                 = 16.16 MiB = 0.02 GiB
      Write bandwidth                    = 47.0496 MiB/sec
      -----------------------------------------------------------
  ```

### Developers
* Wei-keng Liao <<wkliao@northwestern.edu>>
* Kai-yuan Hou <<kai-yuanhou2020@u.northwestern.edu>>

Copyright (C) 2021, Northwestern University.
See [COPYRIGHT](COPYRIGHT) notice in top-level directory.

### Project funding supports:
This research was supported by the Exascale Computing Project (17-SC-20-SC), a
joint project of the U.S. Department of Energy's Office of Science and National
Nuclear Security Administration, responsible for delivering a capable exascale
ecosystem, including software, applications, and hardware technology, to
support the nation's exascale computing imperative.

