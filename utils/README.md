## Utility Programs for E3SM-IO benchmark

* [dat2nc](#dat2nc) -- converts multiple decomposition files into a NetCDF file
* [dat2decom](#dat2decom) -- converts decomposition files into a HDF5/NetCDF4/BP file
* [datstat](#datstat) -- displays statistics of decomposition files.
* [pnetcdf_blob_replay](#pnetcdf_blob_replay) -- converts the subfiles produced
  by the `e3sm_io` benchmark when it ran with PnetCDF blob I/O strategy into a
  regular NetCDF file.

---
## dat2nc
**dat2nc** is a utility program that converts a PIO decomposition file (in a
text format) to a NetCDF file, which can be fed to `e3sm_io` benchmark to
evaluate the I/O performance.

### Convert the data decomposition file into NetCDF file
The instructions below explain how to convert the data decomposition file into
NetCDF file format.
* For the F case, there are three data decomposition files generated by the
  E3SM production runs, The files are in a text format with file extension name
  ".dat". Converting the decomposition files into a single NetCDF file enables
  the parallel reading. For the G case, there are 6 decomposition files that
  need to be converted first. For the I case, there are 5 decomposition files.
* A utility program, `dat2nc.c`, can be used to convert the text files. Running
  command `make` under to root folder should already build the executable.
* In addition to converting the text-based decomposition map into binary form. 
  The dat2nc utility also converts the decomposition map into the format used by
  the E3SM benchmark.
  + **0 offsets are removed** -
    In the PIO decomposition map, the index starts from 1. The decomposition
    map may contain 0s which means the process did not access any element. They
    are removed in the converted NetCDF file.
  + **Access to consecutive locations is merged** -
    The PIO decomposition map records the variable offset of every element
    accessed by a process. The dat2nc utility converts them into offset and
    length pairs, merging consecutive elements.
  + **The offset and length pairs are sorted according to the offset** -
* The original decomposition map can be included in the NetCDF file by adding
  option `-r`. When enabled, dat2nc will save a copy of the original
  decomposition map in the binary form before applying the conversion mentioned
  above. It is used in the ADIOS blob test cases.
* An example of input file is provided [./input_files.txt](input_files.txt).
  ```
    % cat ./input_files.txt
    ../datasets/piodecomp16tasks16io01dims_ioid_514.dat
    ../datasets/piodecomp16tasks16io01dims_ioid_516.dat
    ../datasets/piodecomp16tasks16io02dims_ioid_548.dat
  ```
* The command to combine the three `.dat` files for the F case to a NetCDF file
  for the F case as an example, is:
  ```
    % ./dat2nc -i input_list.txt -o f_case_866x72_16p.nc
  ```
* Command-line options of `./dat2nc`:
  ```
    % ./dat2nc -h
    Usage: ./dat2nc [-h|-v|-r|-l] -i input_file -o out_file
       -h               Print help
       -v               Verbose mode
       -r               Include original decomposition map
       -l num           max number of characters per line in input file
       -i input_file    list of decomposition file names
       -o out_file      name of output NetCDF file
  ```
* Three small input decomposition files for an F case are provide in
  directory [../datasets/](../datasets/).
  * `piodecomp16tasks16io01dims_ioid_514.dat`  (decomposition along the fastest dimensions)
  * `piodecomp16tasks16io01dims_ioid_516.dat`  (decomposition along the fastest dimensions)
  * `piodecomp16tasks16io02dims_ioid_548.dat`  (decomposition along the fastest two dimensions)
* The NetCDF file converted from these 3 decomposition `.dat` files is also
  provided in the same folder `../datasets` with file named
  `f_case_866x72_16p.nc`. Its metadata is shown below.
  ```
    % cd ./datasets
    % ncmpidump -h f_case_866x72_16p.nc
    netcdf f_case_866x72_16p {
    // file format: CDF-1
    dimensions:
        num_decomp = 3 ;
        decomp_nprocs = 16 ;
        D1.total_nreqs = 47 ;
        D2.total_nreqs = 407 ;
        D3.total_nreqs = 29304 ;
    variables:
        int D1.nreqs(decomp_nprocs) ;
            D1.nreqs:description = "Number of noncontiguous requests per process" ;
        int D1.offsets(D1.total_nreqs) ;
            D1.offsets:description = "Flattened starting indices of noncontiguous requests" ;
        int D1.lengths(D1.total_nreqs) ;
            D1.lengths:description = "Lengths of noncontiguous requests" ;
            D1.lengths:max = 36 ;
            D1.lengths:min = 9 ;
        int D2.nreqs(decomp_nprocs) ;
            D2.nreqs:description = "Number of noncontiguous requests per process" ;
        int D2.offsets(D2.total_nreqs) ;
            D2.offsets:description = "Flattened starting indices of noncontiguous requests" ;
        int D2.lengths(D2.total_nreqs) ;
            D2.lengths:description = "Lengths of noncontiguous requests" ;
            D2.lengths:max = 4 ;
            D2.lengths:min = 1 ;
        int D3.nreqs(decomp_nprocs) ;
            D3.nreqs:description = "Number of noncontiguous requests per process" ;
        int D3.offsets(D3.total_nreqs) ;
            D3.offsets:description = "Flattened starting indices of noncontiguous requests" ;
        int D3.lengths(D3.total_nreqs) ;
            D3.lengths:description = "Lengths of noncontiguous requests" ;
            D3.lengths:max = 4 ;
            D3.lengths:min = 1 ;

    // global attributes:
        :command_line = "./dat2nc -o f_case_866x72_16p.nc -1 datasets/piodecomp16tasks16io01dims_ioid_514.dat -2 datasets/piodecomp16tasks16io01dims_ioid_516.dat -3 datasets/piodecomp16tasks16io02dims_ioid_548.dat " ;
        :D1.ndims = 1 ;
        :D1.dims = 866 ;
        :D1.max_nreqs = 4 ;
        :D1.min_nreqs = 2 ;
        :D2.ndims = 1 ;
        :D2.dims = 866 ;
        :D2.max_nreqs = 39 ;
        :D2.min_nreqs = 13 ;
        :D3.ndims = 2 ;
        :D3.dims = 72, 866 ;
        :D3.max_nreqs = 2808 ;
        :D3.min_nreqs = 936 ;
    }
  ```
* The NetCDF file containing the 6 decompositions from a small G case is also
  available in folder `../datasets` with file named `g_case_cmpaso_16p.nc`.
* The NetCDF file containing the 5 decompositions from a small I case is also
  available in folder `../datasets` with file named `i_case_f19_g16_16p.nc`.

## dat2decom
**dat2decom** is generalized from [dat2nc](#dat2nc) that can convert the
decomposition data files into HDF5, NetCDF4, or BP file format, in addition to
the NetCDF classic 64-bit data format.

### Convert the data decomposition file into HDF5/NetCDF4/ADIOS file
The instructions below explain how to convert the data decomposition file into
NetCDF, HDF5, NetCDF4, or ADIOS file format.
* A utility program, `dat2decomp.c`, is an alternative to dat2nc. Running
  command `make` under to root folder should already build the executable.
  + dat2decomp allows the user to specify their desired file format for the 
    converted decomposition file.
  + Use -a option to select the file format.
    + pnetcdf - same as dat2nc
    + hdf5 - store decomposition file in HDF5 format
    + netcdf4 - store decomposition file in NetCDF 4 format
    + adios - store decomposition file in ADIOS2 format
  + 
* The command to combine the three `.dat` files for the F case to a HDF5 file
  for the F case as an example, is:
  ```
    % ./dat2decom -a hdf5 -i input_list.txt -o f_case_866x72_16p.nc
  ```
* Command-line options of `./dat2decom`:
  ```
    % ./dat2decom -h
    Usage: ./dat2decom [-h|-v|-r|-l num] -a api -i input_file -o out_file
       [-h]            Print help
       [-v]            Verbose mode
       [-r]            Include original decomposition map
       [-l num]        max number of characters per line in input file
       -a api          output file format, api is one of the followings
          pnetcdf:     NetCDF classic 64-bit data format
          netcdf4:     NetCDF-4 (HDF5-based) format
          hdf5:        HDF5 format
          adios:       ADIOS2 BP3 format
       -i input_file   list of decomposition file names
       -o out_file     name of output NetCDF file
  ```

---
## datstat
**datstat** reads a decomposition file in its original text format and report
statistics of the decomposition.

* Command-line options of `datstat`:
  ```
    % ./datstat -h
    Usage: ./datstat [OPTION]...
           -h               Print help
           -d input_file    decomposition file to analyze
  ```

* Example run:
  ```
    % ./datstat -d ../datasets/piodecomp16tasks16io01dims_ioid_514.dat
    =========================================================================
    Deomposition file ../datasets/piodecomp16tasks16io01dims_ioid_514.dat
    =========================================================================
    version: 2001
    npes: 16
    ndims: 1
    Dims: 866,
    total_cells: 1536
    total_zero_cells: 670
    max_cells: 96 @ rank 0
    max_zero_cells: 56 @ rank 12
    zero ratio: 0.436198
  ```
---
## bpstat
**bpstat** counts the size of attributes and variables in an ADIOS2 BP3 file.
If the target file is a sub-filed, user should specify the directory name without the .dir extension.

* Command-line options of `datstat`:
  ```
    % ./datstat -h
    Usage: ./datstat [OPTION]...
           -v               Verbose mode
           -h               Print help
           FILE             Name of BP file to analysis
  ```

* Example run:
  ```
    % ./datstat -d ./f_case_h0.bp
    Num subfiles: 1
    Num variables: 1203
    Total variable size: 17115880
    Num attributes: 3501
    Total attribute size: 302738
    Total object size (MiB): 16.6117
    Total object size (GiB): 0.0162224
  ```
---
## pnetcdf_blob_replay
**pnetcdf_blob_replay** reads the subfiles produced by the `e3sm_io` benchmark
into a single, regular NetCDF file. Currently, it must be run with the same
number of MPI processes that produced the subfiles. The future work will remove
such limitation to allow less number of MPI processes to convert.

* Command-line options of `pnetcdf_blob_replay`:
  ```
    % ./pnetcdf_blob_replay -h
      Usage: pnetcdf_blob_replay [OPTION]... FILE
          [-h] Print help
          [-v] Verbose mode
          [-i file] Base name of input subfiles
          [-o file] Output file name
  ```
* Example run:
  ```
    % mpiexec -n 16 ./pnetcdf_blob_replay -i blob_F_out_h0.nc -o F_out_h0.nc
      Input subfile base name            = blob_F_out_h0.nc
      Output file name                   = F_out_h0.nc
      No. decompositions                 =   3
      No. variables                      = 429
      No. records (time steps)           =   1
      Max Time of file open/create       = 0.0494 sec
      Max Time of define variables       = 0.0446 sec
      Max Time of read                   = 0.0090 sec
      Max Time of write                  = 0.3435 sec
      Max Time of close                  = 0.0249 sec
      Max end-to-end time                = 0.4714 sec
      -----------------------------------------------------------
      Total read  amount                 = 16.80 MiB = 0.02 GiB
      Read  bandwidth                    = 1869.3614 MiB/sec
      Total write amount                 = 16.16 MiB = 0.02 GiB
      Write bandwidth                    = 47.0496 MiB/sec
      -----------------------------------------------------------
  ```

### Developers
* Wei-keng Liao <<wkliao@northwestern.edu>>
* Kai-yuan Hou <<kai-yuanhou2020@u.northwestern.edu>>

Copyright (C) 2021, Northwestern University.
See [COPYRIGHT](COPYRIGHT) notice in top-level directory.

### Project funding supports:
This research was supported by the Exascale Computing Project (17-SC-20-SC), a
joint project of the U.S. Department of Energy's Office of Science and National
Nuclear Security Administration, responsible for delivering a capable exascale
ecosystem, including software, applications, and hardware technology, to
support the nation's exascale computing imperative.

